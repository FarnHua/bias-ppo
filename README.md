# Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models

This project is specifically designed to explore and provoke biases in Language Models (LLMs) using Reinforcement Learning techniques. 
For a detailed exploration of our methods and findings, refer to our [paper](https://arxiv.org/abs/2310.11079).

## Prerequisites

Before you begin, ensure you have the following prerequisites installed:
- Python 3.9.18
- Relevant Python libraries as listed in `requirements.txt`

## Installation

To set up Bias-PPO on your local machine, follow these steps:
1. Clone the repository: `git clone https://github.com/your-repository/bias-ppo.git`
2. Navigate to the project directory: `cd bias-ppo`
3. Install the required dependencies: `pip install -r requirements.txt`

## Usage


### Training 
To train the Reinforcement Learning model for bias provocation in LLMs, execute the script below.
Parameters:
* `--path : Data path for updating lm_loss`
* `--model_name : Pretrained test case generator path`
* `--bot : Target LLM`
<details>
<summary>Example Training Script</summary>

```
python main.py \
  --mode finetune \
  --prompt GPT2 \
  --agent ppo_ptx_kl \
  --path <PRETRAIN DATASET> \
  --model_name <MODEL TO TRAIN> \
  --bot <BOT> \
  --dataset Netflix \
  --type bias \
  --exp_name test \
  --log_interval 5\
  --seed 42 \
  --bz 8 \
  --kl_coef 0.05 \
  --ep_lr 1.0 \
  --k_epoch 5\
  --discount_r 1.0 \
  --end_batch 300 \
  --sample_time 8 \
  --max_pt_len 30 \
  --inner_lr 9e-6 \
  --lm_lr 0.01 \
  --init_step 100 \
  --save_path test \
  --save_interval 10 \
  --wandb disabled

```

</details>

### Generate testcases
* `--model_path : Path to the finetuned GPT-2 checkpoint`
* `--bot : Target LLM`

```
python3 test.py \
  --model_path <YOUR MODEL CKPT> \
  --bot <BOT> \
```
* The generated test case files will be located at `./RL_Result/<BOT>/`, including:
  * `<BOT>-distinct1000-test.csv`: Bias-provoking prompts.
  * `<BOT>-distinct1000-incontext.csv`: Example prompts for mitigation.

### Analyzing Test Results
To evaluate the test cases generated by GPT-2, use the following script.
Parameters:
* `--prompt_path : CSV file containing test cases`
* `--save_path : Path for saving results, a CSV file with sentiment gaps of test cases and responses`
* `--bot : LLM under test`
  
```
python3 test.py \
--prompt_path <YOUR PROMPT PATH> \
--save_path <PATH TO SAVE RESULT> \
--bot <BOT> \
```
### Mitigation

#
Modified from [BlackBoxBot](https://github.com/pohanchi/blackboxbot)