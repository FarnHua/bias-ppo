Message from TWCC HPC admin
-----------------------
loading miniconda3 with conda 4.8.4/python 3.7
docs : https://hackmd.io/@kmo/twcc_hpc_conda
-----------------------

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Traceback (most recent call last):
  File "test.py", line 17, in <module>
    model.load_state_dict(torch.load(model_path))
  File "/home/u5273929/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1672, in load_state_dict
    self.__class__.__name__, "\n\t".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for GPT2LMHeadModel:
	Missing key(s) in state_dict: "transformer.h.0.attn.bias", "transformer.h.0.attn.masked_bias", "transformer.h.1.attn.bias", "transformer.h.1.attn.masked_bias", "transformer.h.2.attn.bias", "transformer.h.2.attn.masked_bias", "transformer.h.3.attn.bias", "transformer.h.3.attn.masked_bias", "transformer.h.4.attn.bias", "transformer.h.4.attn.masked_bias", "transformer.h.5.attn.bias", "transformer.h.5.attn.masked_bias", "transformer.h.6.attn.bias", "transformer.h.6.attn.masked_bias", "transformer.h.7.attn.bias", "transformer.h.7.attn.masked_bias", "transformer.h.8.attn.bias", "transformer.h.8.attn.masked_bias", "transformer.h.9.attn.bias", "transformer.h.9.attn.masked_bias", "transformer.h.10.attn.bias", "transformer.h.10.attn.masked_bias", "transformer.h.11.attn.bias", "transformer.h.11.attn.masked_bias", "transformer.h.12.attn.bias", "transformer.h.12.attn.masked_bias", "transformer.h.13.attn.bias", "transformer.h.13.attn.masked_bias", "transformer.h.14.attn.bias", "transformer.h.14.attn.masked_bias", "transformer.h.15.attn.bias", "transformer.h.15.attn.masked_bias", "transformer.h.16.attn.bias", "transformer.h.16.attn.masked_bias", "transformer.h.17.attn.bias", "transformer.h.17.attn.masked_bias", "transformer.h.18.attn.bias", "transformer.h.18.attn.masked_bias", "transformer.h.19.attn.bias", "transformer.h.19.attn.masked_bias", "transformer.h.20.attn.bias", "transformer.h.20.attn.masked_bias", "transformer.h.21.attn.bias", "transformer.h.21.attn.masked_bias", "transformer.h.22.attn.bias", "transformer.h.22.attn.masked_bias", "transformer.h.23.attn.bias", "transformer.h.23.attn.masked_bias", "transformer.h.24.attn.bias", "transformer.h.24.attn.masked_bias", "transformer.h.25.attn.bias", "transformer.h.25.attn.masked_bias", "transformer.h.26.attn.bias", "transformer.h.26.attn.masked_bias", "transformer.h.27.attn.bias", "transformer.h.27.attn.masked_bias", "transformer.h.28.attn.bias", "transformer.h.28.attn.masked_bias", "transformer.h.29.attn.bias", "transformer.h.29.attn.masked_bias", "transformer.h.30.attn.bias", "transformer.h.30.attn.masked_bias", "transformer.h.31.attn.bias", "transformer.h.31.attn.masked_bias", "transformer.h.32.attn.bias", "transformer.h.32.attn.masked_bias", "transformer.h.33.attn.bias", "transformer.h.33.attn.masked_bias", "transformer.h.34.attn.bias", "transformer.h.34.attn.masked_bias", "transformer.h.35.attn.bias", "transformer.h.35.attn.masked_bias". 
